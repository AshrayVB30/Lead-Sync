# Lead Sync + AI Notes System

A full-stack application that fetches leads from a public API, allows adding notes per lead, and generates **AI-powered summaries using Ollama** for those notes.

---

## ğŸš€ Tech Stack

### Backend
- **Python 3.12**
- **FastAPI** - Modern async web framework
- **Uvicorn** - ASGI server
- **Ollama (phi3:mini)** - Local LLM for AI summaries
- **JSON file persistent store** (demo storage layer)
- **Pydantic** - Data validation

### Frontend
- **Next.js 14** (App Router)
- **TypeScript**
- **React** with modern hooks
- **CSS Modules** for styling

### AI Integration
- **Ollama phi3:mini** for local LLM inference (2.2GB)
- AI summary generation (max 20 words)
- Automatic fallback if Ollama unavailable
- Swappable AI provider architecture

---

## ğŸ§© Features

### 1ï¸âƒ£ Fetch Leads
- Fetches leads from: `https://jsonplaceholder.typicode.com/users`
- Extracts: **name**, **email**, **phone**
- Served via backend proxy (`/leads` endpoint)
- Beautiful card-based display

### 2ï¸âƒ£ Add Notes
- Each lead has an "Add Notes" button
- Opens modal for note entry
- Notes are persisted in JSON file store
- Notes mapped by email (unique key)

### 3ï¸âƒ£ AI Summary with Ollama ğŸ¦™
- **Generates summary using Ollama phi3:mini**
- Enforced max 20 words
- Saved alongside note
- AI service abstracted for easy provider swap
- Automatic fallback if Ollama unavailable

---

## ğŸ›  Setup Instructions

### Prerequisites
- Python 3.12+
- Node.js 18+
- **Ollama** ([Download here](https://ollama.com/download))

---

### ğŸ”¹ Step 1: Install Ollama

1. **Download and install Ollama** from: https://ollama.com/download
2. **Pull the model:**
   ```bash
   ollama pull phi3:mini
   ```
3. **Verify Ollama is running:**
   ```bash
   ollama list
   ```

---

### ğŸ”¹ Step 2: Backend Setup

```bash
# Create conda environment
conda create -n fullstack python=3.12
conda activate fullstack

# Install dependencies
cd backend
pip install -r requirements.txt

# Run server
uvicorn main:app --reload
```

**Backend runs on:** `http://localhost:8000`  
**Swagger Docs:** `http://localhost:8000/docs`

---

### ğŸ”¹ Step 3: Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Run development server
npm run dev
```

**Frontend runs on:** `http://localhost:3000`

---

## ğŸ“¡ API Endpoints

### `GET /leads`
Returns cleaned list of leads.

### `POST /notes`
Creates note with AI summary (using Ollama).

**Request:**
```json
{
  "email": "example@email.com",
  "note": "Your note text here"
}
```

**Response:**
```json
{
  "email": "example@email.com",
  "note": "Your note text here",
  "summary": "AI-generated summary (max 20 words)"
}
```

### `POST /summary`
Generates standalone summary using Ollama.

---

## ğŸ¦™ Ollama Configuration

**Current model:** `phi3:mini` (2.2GB)

**To change the model**, edit [`backend/services/ai_service.py`](backend/services/ai_service.py):
```python
OLLAMA_MODEL = "phi3:mini"  # Change to any pulled model
```

**Alternative models:**
- `tinyllama` - Smallest (637MB)
- `llama3.2` - Larger, needs more RAM
- `mistral` - Good alternative

---

## ğŸ— Project Structure

```
backend/
â”œâ”€â”€ main.py                 # FastAPI app with CORS
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schemas.py         # Pydantic models
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ leads.py           # GET /leads
â”‚   â””â”€â”€ notes.py           # POST /notes, POST /summary
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ leads_service.py   # External API integration
â”‚   â””â”€â”€ ai_service.py      # Ollama AI integration
â””â”€â”€ storage/
    â””â”€â”€ json_store.py      # JSON persistence

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/               # Next.js pages
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”œâ”€â”€ lib/               # API client
â”‚   â””â”€â”€ types/             # TypeScript types
â””â”€â”€ package.json
```

---

## ğŸ§  Design Decisions

### Backend Proxy Pattern
Leads are fetched through backend to:
- Avoid CORS issues
- Maintain clean API abstraction
- Enable future caching

### Storage Abstraction
JSON persistent store for demo simplicity.  
Storage layer is abstracted for easy replacement with SQLite/PostgreSQL.

### Service Layer Pattern
Business logic separated from route handlers for better maintainability and testability.

### AI Layer Isolation
AI logic in `ai_service.py` allows switching between providers (Ollama, OpenAI, Claude) without route changes.

---

## ğŸ“ˆ Scalability Considerations

If productionized:

âœ… Replace JSON store with **PostgreSQL**  
âœ… Add **authentication layer** (JWT)  
âœ… Introduce **caching** for leads (Redis)  
âœ… Add **background job queue** for AI generation (Celery)  
âœ… Add **optimistic UI updates**  
âœ… Add **pagination** for large lead sets  

---

## ğŸ¨ UI Features

- **Premium gradient design** with purple-violet theme
- **Smooth animations** (hover effects, modal transitions)
- **Responsive grid layout** for leads
- **Loading states** with spinner
- **Error handling** with retry
- **Stats dashboard** showing metrics
- **AI summary** with sparkle icon âœ¨

---

## ğŸ”§ Troubleshooting

**Issue: AI summaries not working**
- Ensure Ollama is running: `ollama list`
- Check model is pulled: `ollama pull phi3:mini`
- Verify port 11434 is accessible

**Issue: CORS errors**
- Backend must run on port 8000
- Frontend must run on port 3000

**Issue: Slow AI responses**
- First request loads model (5-10s)
- Subsequent requests faster (1-3s)

---

## ğŸ“š Documentation

- **Setup Guide:** This README
- **Ollama Setup:** [`backend/OLLAMA_SETUP.md`](backend/OLLAMA_SETUP.md)
- **API Docs:** `http://localhost:8000/docs` (when backend running)
- **Testing Guide:** [`TESTING_GUIDE.md`](TESTING_GUIDE.md)

---

## âœ¨ Features Implemented

âœ… Fetch leads from external API  
âœ… Display leads in card grid  
âœ… Add notes to leads  
âœ… AI-powered summaries (Ollama)  
âœ… Persistent storage (JSON)  
âœ… Auto-generated API docs  
âœ… TypeScript for type safety  
âœ… Responsive design  
âœ… Loading states  
âœ… Error handling  
âœ… Premium UI/UX  

---

Made with â¤ï¸ using FastAPI, Next.js, and Ollama ğŸ¦™
